{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the data\n",
    "import os\n",
    "import pandas as pd\n",
    "import data_utility as du\n",
    "\n",
    "# setup pathing and names for the resources that needs to be downloaded and extracted\n",
    "SRC_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz\"\n",
    "DEST_ROOT = \"E:\\machine-learning-dataset\"\n",
    "TAR_FILE_NAME = \"housing.tgz\"\n",
    "TAR_FILE_PATH = os.path.join(DEST_ROOT, TAR_FILE_NAME)\n",
    "EXTRACTED_DEST_PATH = os.path.join(DEST_ROOT, \"housing\")\n",
    "CSV_FILE = os.path.join(EXTRACTED_DEST_PATH, \"housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.get_data_from_url(SRC_URL, DEST_ROOT, TAR_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.extract_tarfile(TAR_FILE_PATH, EXTRACTED_DEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# see top 5 rows example code\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick description of the data example code\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate count example code\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of numerical attribute\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2. Review the data. Check for capped values in histograms and evaluate with teams that will use your ml.\n",
    "# if it does, either collect proper labels for the set whos labels are capped or remove those set.\n",
    "\n",
    "# plot a histogram for each numerical attribute\n",
    "# %matplotlib inline shows plot inside jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# create histogram. hist method is only available with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "# this method is optional. housing.hist() showns this automatically, but it includes the array.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: split the data into train, and test. Do not look at the test set. This is to avoid ovefitting if your brain detects pattern from the entire data set. Recommendation is to use option c) stratified method to split.\n",
    "\n",
    "# option a) generating train, test set using random single run\n",
    "train_set, test_set = du.split_train_test_single_run(housing, 0.2)\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option b-1) using the re-runable method that uses hashing off an index column\n",
    "# generate an index column first\n",
    "housing_with_index = housing.reset_index()\n",
    "housing_with_index.head()\n",
    "\n",
    "# the trick is to make sure new data gets appended to the end, for this hash algorithm method to work\n",
    "train_set, test_set = du.split_train_test_multi_run(housing_with_index, 0.2, \"index\")\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option b-2) Alternatively use sci kit framework method\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this is the same thing as split_train_test_single_run\n",
    "# passing random_state will allow setting a random seed. \n",
    "# you can also pass it multiple data sets incase you separated the label in another dataframe\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option c) We however want to use strafied sampling instead of random.\n",
    "\n",
    "# Divide into homogenous subgrous called strata, and the right number of instances are sampled from each stratum to \n",
    "# guarantee the test is representative of the entire dataset\n",
    "# we have to categorize things based on mediam_income since most people think it's an important category for predicting median house prices.\n",
    "import numpy\n",
    "housing[\"median_income\"].hist()\n",
    "housing[\"income_category\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., numpy.inf], labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_category\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# n_splits = Number of re-shuffling & splitting iterations, \n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# only 1 iteration instance\n",
    "for train_index, test_index in split.split(housing, housing[\"income_category\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the sample is as expected\n",
    "strat_test_set[\"income_category\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the category that created for stratify, since it's no longer needed.\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_category\", axis=1, inplace=True) # inplace means mutable. default is inplace=false which only affects the copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Discovery and Visualize data to gain insights\n",
    "# create a copy, so we don't affect the training set when we play with data visualization\n",
    "housing = strat_train_set.copy()\n",
    "\n",
    "# 4.1 - Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above plot looks like california but, but density isn't show well. \n",
    "# adjust by lowing the alpha to get a heatmap feel\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the brain is very good at splotting patterns in pictures, we just have to play with different visualization parameters to make patterns stand out\n",
    "# s = radius of the circle => population\n",
    "# c = color => price\n",
    "# cmp = predefined colour map => jet (ranges from blue = low values to red = high values)\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 - Look for correlations\n",
    "# for none large datasets, easily compute the standard correlation coefficient (Pearson's r) between every pair of attributes using the correlatioon method:\n",
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example, check correlation of median_house_value with all values\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to check correlation is to visualize it all using pandas scatter_matrix() function\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most promising attribute is median income. zoom in on it as an example.\n",
    "# there is a ceiling due to $500,000 cap on dataset. \n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 - Experiment with attribute combinations\n",
    "# Identified few data quirks that should be cleaned up before feeidng the data to ML algorithm. Some algorith also has tail heavy distribution. We may want to trasnform them by computing their logarithm. This is done in step 7.abs\n",
    "\n",
    "# Before preparing the data for ML, try out various attribute combinatioons. Eg, total numbr of bedrooms ina district is not very useful if we dont know how many households there are.\n",
    "# Total number of bedrooms by itself is not very useful, we want to compare it to numbr of rooms.\n",
    "# Population per household also seems like an interesting attribute combination to look at.abs\n",
    "\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "# Look at correlation matrix again\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "# notice that bedrooms_per_room that was calculate has better correlation than total_rooms. \n",
    "# We can now notice that house with lower bedroom/room ratio tend to be more expensive\n",
    "\n",
    "# number of rooms per household also shows more info than total number of rooms in a district. larger house => more expensive.\n",
    "\n",
    "# this round of exploration does not have to be absolutely thorough. it's just to gain quick insights that can help with a good protoype. This is a iterative process.\n",
    "\n",
    "#QUESTION TODO: why does this matter when the raw new data is a set of data that's derived from 2 existing data that's used by the model. Are the columns in dataset treated as independent during ML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 Preparing the data for ML algorithm\n",
    "# Take this opportunity to build a library of transformation functions for reuse\n",
    "\n",
    "# first revert to a clean train set by taking the last strat_train set snapshot. and removing some unwanted columns\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # axis=1 is equivalent to columns=labels. inplace is false, this only affects the copy.\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# 5.1 - Data cleaning\n",
    "# Most ML cannot work with missing features. create a few functions to take care of them.\n",
    "\n",
    "# We saw total_bedrooms attribute has some missing values.\n",
    "# option 1 - Get rid of the corresponding districts.\n",
    "# option 2 - Get rid of the whole attibute.\n",
    "# option 3 - Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "# housing.dropna(subset=[\"total_bedrooms\"]) option 1\n",
    "\n",
    "# housing.drop(\"total_bedrooms\", axis=1) option 2\n",
    "\n",
    "# median = housing[\"total_bedrooms\"].median() # option 3\n",
    "# housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "# if option 3 is taken, a median value is computed. make usre to save it so we can use it to replace missing values in the test set when we want to evaluate the system.\n",
    "# We can use SK library imputer to do option 3 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3 using Sci-Kit\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# drop ocean_priximity first because median can only be computed on numerical attributes.\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# computer the median of each atribute, and store in statistics_ instance\n",
    "imputer.fit(housing_num)\n",
    "# take a look at statistics_ and see how it basically stored all the median values\n",
    "imputer.statistics_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "# X is a NumPy array. put it back into a pandas DataFrame\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.2 - Handling text and categorical attributes\n",
    "# Move on to deal with text attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most ml prefer to numbers. convert these categories from text to numbers.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get list of categories using categories_ instance variable => a list containing a 1D array of categories for each categorical attribute. \n",
    "# (one categorical attribute => single array)\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a problem with this rep is ml will assume two nearby values are more similar than two distance values.\n",
    "# this might be ok for some cases such such as order categories 'bad','average','good','excellent'\n",
    "# but for ocean proximity. eg, categories 0 and 4 are more similar than categories 0 and 1\n",
    "# To fix this, create one binary attribute per category:\n",
    "# one attribute equal to 1 when category is <1H OCEAN, 0 otherwise\n",
    "# one attribute equal to 1 when cateogry is INLAND, 0 otherwise. and so on.\n",
    "# this is called one-hot encoding. 1 attribute will equal 1 (hot) while others will be 0 (cold)\n",
    "# the new attributes are sometimes called dummy attributes. Scikit-Learn provide OneHotEncoder class to convert categorical values to one-hot vector\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output is a SciPy sparse matrix instead of NumPy array. Very useful when you have categorical attriobutes with thousands of categories.\n",
    "# (This is sparse matrix is more space efficient because it only stores location of the nonzero elements). We can use NumPy but it's expensive.\n",
    "\n",
    "# eg: if you really want NumPy aray, call toarray()\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg: can see categories with categories_ property\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if categorical attribute has a large number of possible categories, then one-hot encoding will result in a large number of inputs that can slowdown performance. \n",
    "# In this case, replace categorical input with useful numerical features like replacing ocean_proximity to distance to ocean\n",
    "\n",
    "# 5.3 - Custom Transformer\n",
    "# see transform.py for example\n",
    "import transform\n",
    "attr_adder = transform.CombinedAttributesAdder(add_bedrooms_per_room=False) \n",
    "# this example has one hyperparameter => add_bedrooms_per_room set to True by default. \n",
    "# (hyperparameter is a parameter whose value is used to control ml process. In this case allow us to easily findout whether adding this attribute helps the ml algorithm)\n",
    "# More specifically we can take advantage of hyperparameters to gate any data prep step we're not sure about\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "# 5.4 - Feature scaling\n",
    "# ML algorithms don't perform well when the input numerical values have very different scales. \n",
    "# Example, this is the case for housing data where total number of rooms range from 6 to 39,320 while median income only range from 0 to 15.abs\n",
    "# Note that scaling the target values is generally not required.abs\n",
    "# 2 Ways to get all attributes in same scale\n",
    "# a) min-max scaling - (normalization) values are shifted and rescaled so theye nd up ranging from 0 to 1. (value - min) / (max - min)\n",
    "#    Sci-kit MinMaxScalar, which comes with hyperparameter feature_range if we want to scale beyond 0 to 1\n",
    "# b) standardization - subtract the mean value so standardized values always have zero mean, then it divides by the standard deviation so that the resulting distribution has unit variance\n",
    "#    standardization is not bound like min-max, which may be a problem for some ml algorithms like neural networks.\n",
    "#    Sci-kit has StandardScalar \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 - Transformation Pipelines\n",
    "# Use Scikit-Learn pipeline class to help with sequence of pipelines\n",
    "# Put all the transform stuff in a pipeline instead\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# constructor takes names/estimator pairs. All but last one must all be transformers (with fit_transform)\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                         ('attribs_adder', transform.CombinedAttributesAdder()),\n",
    "                         ('std_scaler', StandardScaler()),\n",
    "                         ])\n",
    "\n",
    "# example\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci-kit has column transformer \n",
    "# use one transformer pipeline to transform multiple columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "# note that num_pipeline returns a dense matrix, but OneHotEncoder returns a sparse matrix. \n",
    "# The columnTransformer estimates the density of the final matrix and returns a sparse matrix if the density is\n",
    "# lower than a given threshold. default => sparse_trshold=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Select and Train a Model\n",
    "# 6.1  -Training and evaluating on the training set\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# try out on few instance from training set\n",
    "first_five_data = housing.iloc[:5]\n",
    "first_five_labels = housing_labels.iloc[:5]\n",
    "first_five_data_prepared = full_pipeline.transform(first_five_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(first_five_data_prepared))\n",
    "print(\"Labels:\", list(first_five_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 - Measure accuracy\n",
    "# Using RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = numpy.sqrt(lin_mse)\n",
    "lin_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering district medians are between $120,000 and $265,000, this is not good enough.\n",
    "# this could be because the features do not provide enough info, or the model is not powerful enough\n",
    "# main ways to fix underfitting =>\n",
    "# 1. choose a more powerful model\n",
    "# 2. feed the training algorith more features\n",
    "# 3. reduce the constraints on the model\n",
    "\n",
    "# 6.3 Trying a more powerful model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor # A powerful model that can find complex nonlinear relationships in the data\n",
    "tree_reg = DecisionTreeRegressor() \n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = numpy.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "# that model likely overfit the data because the we're testing with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 - Better evaluation using cross-validation\n",
    "# Option 1. Use the train_test_split() function to split the training set into a smaller training set and a validation set, and train the model against the smaller training set.\n",
    "# Option 2. Use Scikit-Learn's K-fold cross-validation feature to randomly split the training set into 10 distinct subsets called folds then train and evaluate the decision tree model 10 times picking a different fold for evaluation each time and training on the 9 folds. result => aray containing 10 evaluation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = numpy.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = numpy.sqrt(-lin_scores)\n",
    "du.display_scores(lin_rmse_scores)\n",
    "\n",
    "# this decision tree is overfitted and is performing worse than linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 - Try random forest model\n",
    "# Random forest works by training many decision trees on random subsets of features, then average their predictions. \n",
    "# Building on top of many models => Ensemble Learning\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "random_forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "random_forest_rmse_scores = numpy.sqrt(-random_forest_scores)\n",
    "du.display_scores(random_forest_rmse_scores)\n",
    "\n",
    "# this looks better, but training set score < validation score => overfitting\n",
    "# possible solutions for overfitting\n",
    "# a) simplify model\n",
    "# b) contrain or regularize it\n",
    "# c) get alot more training data\n",
    "\n",
    "# try some more models like Support vector Machoine, or neural network. short-list a few good ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7. Save every model\n",
    "# make sure to save both trained pamaters and hyperparameters, cross-validation scores, and actual predictions so we can easily compare them\n",
    "import joblib\n",
    "joblib.dump(lin_reg, \"california_sensus_lin_reg_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_sesus_lin_reg_model_loaded = joblib.load(\"california_sensus_lin_reg_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8. Fine-tune model\n",
    "# 8.1 grid search or 8.2 randomized search or 8.3 Ensemble Methods\n",
    "# 8.1 - Grid search \n",
    "# a) you can fiddle with hyperparameters manually until a good combination is found (take long time)\n",
    "# b) use grid-search to automatically search it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# example:\n",
    "# specify which hyperparameters to experiment with and what values to try out. it will use cross-=validate to evaluate all the possible combinations of hyperparameter values.\n",
    "\n",
    "# search the best combination of hyperparameter values for RandomForestRegressor\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get best estimator directly\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GridSearchSV is initialized with refit=True (default), once it finds the best estimator using cross-validation, it will retrain on the whole training set.\n",
    "\n",
    "# evaluation scores are also available for viewing.\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "        print(numpy.sqrt(-mean_score), params)\n",
    "\n",
    "# can see after tuning, we get 49,682 using n_estimator hyperparameter = 30, and max_Features hyperparameter =8. it is beter than the default 50,182\n",
    "# you can also treat some data preparation steps are hyperparamters. grid search wil automatically find out whether or not to add a feature like add_bedrooms_per_room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 - Randomized search\n",
    "# Grid search is good for few combinations of hyperparameters\n",
    "# For larger sets of hyperparameters, use RandomizedSearchCV, which evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\n",
    "\n",
    "# 8.3 - Ensemble MEthods\n",
    "# Another way to fine tune is to try combining the models that perform best such as using randomforest instead of iondividual decisiontrees\n",
    "\n",
    "# 8.4 - Analyze the best models and their errors\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances\n",
    "\n",
    "# display these importance scores next to their corrsponding attribute names\n",
    "extra_attribs = [\"rooms_per_house_hold\", \"pop_per_house_hold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n",
    "\n",
    "# You may want to try dropping some of the less useful features (only one ocean_proximity category is really useful)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9. Evaluate the system on the test set\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1) # auto copy\n",
    "y_test = strat_test_set[\"median_house_value\"].copy() # explictly copy\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = numpy.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "numpy.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}