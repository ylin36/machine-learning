{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the data\n",
    "import os\n",
    "import pandas as pd\n",
    "import data_utility as du\n",
    "\n",
    "# setup pathing and names for the resources that needs to be downloaded and extracted\n",
    "SRC_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz\"\n",
    "DEST_ROOT = \"C:\\machine-learning-dataset\"\n",
    "TAR_FILE_NAME = \"housing.tgz\"\n",
    "TAR_FILE_PATH = os.path.join(DEST_ROOT, TAR_FILE_NAME)\n",
    "EXTRACTED_DEST_PATH = os.path.join(DEST_ROOT, \"housing\")\n",
    "CSV_FILE = os.path.join(EXTRACTED_DEST_PATH, \"housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.get_data_from_url(SRC_URL, DEST_ROOT, TAR_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.extract_tarfile(TAR_FILE_PATH, EXTRACTED_DEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# see top 5 rows example code\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick description of the data example code\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate count example code\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of numerical attribute\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2. Review the data. Check for capped values in histograms and evaluate with teams that will use your ml.\n",
    "# if it does, either collect proper labels for the set whos labels are capped or remove those set.\n",
    "\n",
    "# plot a histogram for each numerical attribute\n",
    "# %matplotlib inline shows plot inside jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# create histogram. hist method is only available with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "# this method is optional. housing.hist() showns this automatically, but it includes the array.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: split the data into train, and test. Do not look at the test set. This is to avoid ovefitting if your brain detects pattern from the entire data set. Recommendation is to use option c) stratified method to split.\n",
    "\n",
    "# option a) generating train, test set using random single run\n",
    "train_set, test_set = du.split_train_test_single_run(housing, 0.2)\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option b-1) using the re-runable method that uses hashing off an index column\n",
    "# generate an index column first\n",
    "housing_with_index = housing.reset_index()\n",
    "housing_with_index.head()\n",
    "\n",
    "# the trick is to make sure new data gets appended to the end, for this hash algorithm method to work\n",
    "train_set, test_set = du.split_train_test_multi_run(housing_with_index, 0.2, \"index\")\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option b-2) Alternatively use sci kit framework method\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this is the same thing as split_train_test_single_run\n",
    "# passing random_state will allow setting a random seed. \n",
    "# you can also pass it multiple data sets incase you separated the label in another dataframe\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option c) We however want to use strafied sampling instead of random.\n",
    "\n",
    "# Divide into homogenous subgrous called strata, and the right number of instances are sampled from each stratum to \n",
    "# guarantee the test is representative of the entire dataset\n",
    "# we have to categorize things based on mediam_income since most people think it's an important category for predicting median house prices.\n",
    "import numpy\n",
    "housing[\"median_income\"].hist()\n",
    "housing[\"income_category\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., numpy.inf], labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_category\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# n_splits = Number of re-shuffling & splitting iterations, \n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# only 1 iteration instance\n",
    "for train_index, test_index in split.split(housing, housing[\"income_category\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the sample is as expected\n",
    "strat_test_set[\"income_category\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the category that created for stratify, since it's no longer needed.\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_category\", axis=1, inplace=True) # inplace means mutable. default is inplace=false which only affects the copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Discovery and Visualize data to gain insights\n",
    "# create a copy, so we don't affect the training set when we play with data visualization\n",
    "housing = strat_train_set.copy()\n",
    "\n",
    "# 4.1 - Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above plot looks like california but, but density isn't show well. \n",
    "# adjust by lowing the alpha to get a heatmap feel\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the brain is very good at splotting patterns in pictures, we just have to play with different visualization parameters to make patterns stand out\n",
    "# s = radius of the circle => population\n",
    "# c = color => price\n",
    "# cmp = predefined colour map => jet (ranges from blue = low values to red = high values)\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 - Look for correlations\n",
    "# for none large datasets, easily compute the standard correlation coefficient (Pearson's r) between every pair of attributes using the correlatioon method:\n",
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example, check correlation of median_house_value with all values\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to check correlation is to visualize it all using pandas scatter_matrix() function\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most promising attribute is median income. zoom in on it as an example.\n",
    "# there is a ceiling due to $500,000 cap on dataset. \n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 - Experiment with attribute combinations\n",
    "# Identified few data quirks that should be cleaned up before feeidng the data to ML algorithm. Some algorith also has tail heavy distribution. We may want to trasnform them by computing their logarithm. This is done in step 7.abs\n",
    "\n",
    "# Before preparing the data for ML, try out various attribute combinatioons. Eg, total numbr of bedrooms ina district is not very useful if we dont know how many households there are.\n",
    "# Total number of bedrooms by itself is not very useful, we want to compare it to numbr of rooms.\n",
    "# Population per household also seems like an interesting attribute combination to look at.abs\n",
    "\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "# Look at correlation matrix again\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "# notice that bedrooms_per_room that was calculate has better correlation than total_rooms. \n",
    "# We can now notice that house with lower bedroom/room ratio tend to be more expensive\n",
    "\n",
    "# number of rooms per household also shows more info than total number of rooms in a district. larger house => more expensive.\n",
    "\n",
    "# this round of exploration does not have to be absolutely thorough. it's just to gain quick insights that can help with a good protoype. This is a iterative process.\n",
    "\n",
    "#QUESTION TODO: why does this matter when the raw new data is a set of data that's derived from 2 existing data that's used by the model. Are the columns in dataset treated as independent during ML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 Preparing the data for ML algorithm\n",
    "# Take this opportunity to build a library of transformation functions for reuse\n",
    "\n",
    "# first revert to a clean train set by taking the last strat_train set snapshot. and removing some unwanted columns\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # axis=1 is equivalent to columns=labels. inplace is false, this only affects the copy.\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# 5.1 - Data cleaning\n",
    "# Most ML cannot work with missing features. create a few functions to take care of them.\n",
    "\n",
    "# We saw total_bedrooms attribute has some missing values.\n",
    "# option 1 - Get rid of the corresponding districts.\n",
    "# option 2 - Get rid of the whole attibute.\n",
    "# option 3 - Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "# housing.dropna(subset=[\"total_bedrooms\"]) option 1\n",
    "\n",
    "# housing.drop(\"total_bedrooms\", axis=1) option 2\n",
    "\n",
    "# median = housing[\"total_bedrooms\"].median() # option 3\n",
    "# housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "# if option 3 is taken, a median value is computed. make usre to save it so we can use it to replace missing values in the test set when we want to evaluate the system.\n",
    "# We can use SK library imputer to do option 3 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3 using Sci-Kit\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# drop ocean_priximity first because median can only be computed on numerical attributes.\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# computer the median of each atribute, and store in statistics_ instance\n",
    "imputer.fit(housing_num)\n",
    "# take a look at statistics_ and see how it basically stored all the median values\n",
    "imputer.statistics_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "# X is a NumPy array. put it back into a pandas DataFrame\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5.2 - Handling text and categorical attributes\n",
    "# Move on to deal with text attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most ml prefer to numbers. convert these categories from text to numbers.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get list of categories using categories_ instance variable => a list containing a 1D array of categories for each categorical attribute. \n",
    "# (one categorical attribute => single array)\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a problem with this rep is ml will assume two nearby values are more similar than two distance values.\n",
    "# this might be ok for some cases such such as order categories 'bad','average','good','excellent'\n",
    "# but for ocean proximity. eg, categories 0 and 4 are more similar than categories 0 and 1\n",
    "# To fix this, create one binary attribute per category:\n",
    "# one attribute equal to 1 when category is <1H OCEAN, 0 otherwise\n",
    "# one attribute equal to 1 when cateogry is INLAND, 0 otherwise. and so on.\n",
    "# this is called one-hot encoding. 1 attribute will equal 1 (hot) while others will be 0 (cold)\n",
    "# the new attributes are sometimes called dummy attributes. Scikit-Learn provide OneHotEncoder class to convert categorical values to one-hot vector\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output is a SciPy sparse matrix instead of NumPy array. Very useful when you have categorical attriobutes with thousands of categories.\n",
    "# (This is sparse matrix is more space efficient because it only stores location of the nonzero elements). We can use NumPy but it's expensive.\n",
    "\n",
    "# eg: if you really want NumPy aray, call toarray()\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg: can see categories with categories_ property\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if categorical attribute has a large number of possible categories, then one-hot encoding will result in a large number of inputs that can slowdown performance. \n",
    "# In this case, replace categorical input with useful numerical features like replacing ocean_proximity to distance to ocean\n",
    "\n",
    "# 5.3 - Custom Transformers\n",
    "# see transform.py for example\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) \n",
    "# this example has one hyperparameter => add_bedrooms_per_room set to True by default. \n",
    "# (hyperparameter is a parameter whose value is used to control ml process. In this case allow us to easily findout whether adding this attribute helps the ml algorithm)\n",
    "# More specifically we can take advantage of hyperparameters to gate any data prep step we're not sure about\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "# 5.4 - Feature scaling\n",
    "# ML algorithms don't perform well when the input numerical values have very different scales. \n",
    "# Example, this is the case for housing data where total number of rooms range from 6 to 39,320 while median income only range from 0 to 15.abs\n",
    "# Note that scaling the target values is generally not required.abs\n",
    "# 2 Ways to get all attributes in same scale\n",
    "# a) min-max scaling - (normalization) values are shifted and rescaled so theye nd up ranging from 0 to 1. (value - min) / (max - min)\n",
    "#    Sci-kit MinMaxScalar, which comes with hyperparameter feature_range if we want to scale beyond 0 to 1\n",
    "# b) standardization - subtract the mean value so standardized values always have zero mean, then it divides by the standard deviation so that the resulting distribution has unit variance\n",
    "#    standardization is not bound like min-max, which may be a problem for some ml algorithms like neural networks.\n",
    "#    Sci-kit has StandardScalar \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 - Transformation Pipelines\n",
    "# Use Scikit-Learn pipeline class to help with sequence of pipelines\n",
    "# Put all the transform stuff in a pipeline instead\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# constructor takes names/estimator pairs. All but last one must all be transformers (with fit_transform)\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                         ('attribs_adder', CombinedAttributesAdder()),\n",
    "                         ('std_scaler', StandardScaler)),\n",
    "                         ])\n",
    "\n",
    "# example\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci-kit has column transformer \n",
    "# use one transformer pipeline to transform multiple columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "# note that num_pipeline returns a dense matrix, but OneHotEncoder returns a sparse matrix. \n",
    "# The columnTransformer estimates the density of the final matrix and returns a sparse matrix if the density is\n",
    "# lower than a given threshold. default => sparse_trshold=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Select and Train a Model\n",
    "# 6.1  -Training and evaluating on the training set\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing prepared, housing_labels)\n",
    "\n",
    "# try out on few instance from training set\n",
    "some_Data = housing.iloc[:5]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}